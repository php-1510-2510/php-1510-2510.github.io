---
title       : Regression Basics Part 2
author      : Adam J Sullivan 
job         : Assistant Professor of Biostatistics
work        : Brown University
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js # {highlight.js, prettify, highlight}
hitheme     :  github     # 
widgets     : [mathjax, quiz, bootstrap, interactive] # {mathjax, quiz, bootstrap}
ext_widgets : {rCharts: [libraries/nvd3, libraries/leaflet, libraries/dygraphs]}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
logo        : publichealthlogo.png
biglogo     : publichealthlogo.png
assets      : {assets: ../../assets}
---  .segue bg:grey
```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
knitr::opts_chunk$set(cache=FALSE)
library(ggplot2)
library(fivethirtyeight)
require(tidyverse)
library(broom)
```



# One Continuous 


--- .class #id

## One Continuous Covariate

- We will consider one continuous covariate. 
- We will consider year. 


--- .class #id


## Example: Year and Appearances

- Consider the effect of year on appearances. 
- With categorical data we plotted this with box-whisker plots. 
- We can now use a scatter plot

--- .class #id


## Scatter Plot: Year and Appearances



```{r, eval=F}
ggplot(comic_characters, aes(year, appearances)) + 
  geom_point() + 
  geom_smooth(method="lm") + 
  xlab("Year") + 
  ylab("Appearances")
```


--- .class #id

## Scatter Plot: Year and Appearances



```{r, echo=F}
ggplot(comic_characters, aes(year, appearances)) + 
  geom_point() + 
  geom_smooth(method="loess") + 
  xlab("Year") + 
  ylab("Appearances")
```


--- .class #id



## Modeling What We See

- There might not be a connection or there might be a very small one, let's explore further.
- How can we do this? 
- How does linear regression work?


--- .class #id





## How do we Quantify this?

- One way we could quantify this is
$$\mu_{y|x} = \beta_0 + \beta_1X$$
- where
    - $\mu_{y|x}$ is the mean time for those whose year is $x$. 
    - $\beta_0$ is the $y$-intercept (mean value of $y$ when $x=0$, $\mu_y|0$)
    - $\beta_1$ is the slope (change in mean value of $Y$ corresponding to 1 unit increase in $x$).

--- .class #id



## Population Regression Line

- With the population regression line we have that the distribution of appearances for those at a particular year, $x$, is approximately normal with mean, $\mu_{y|x}$, and standard deviation, $\sigma_{y|x}$. 


--- .class #id


## Population Regression Line

![Distribution of Y and different levels of X.](../Notes/images/Pic1.png)


--- .class #id

## Population Regression Line

- This shows the scatter about the mean due to natural variation. To accommodate this scatter we fit a regression model with 2 parts:
    - Systematic Part
    - Random Part

--- .class #id


## The Model

- This leads to the model
$$Y = \beta_0 + \beta_1X + \varepsilon$$
- Where $\beta_0+\beta_1X$ is the systematic part of the model and implies that $$E(Y|X=x) = \mu_{y|x} = \beta_0 + \beta_1x$$
- the variation part where we have $\varepsilon\sim N(0,\sigma^2)$ which is independent of $X$. 


--- .class #id

## What do We Have?

- Consider the scenario where we have $n$ subjects and for each subject we have the data points $(x,y)$. 
- This leads to us having data in the form $(X_i,Y_i)$ for $i=1,\ldots,n$.
- Then we have the model:
  $$Y_i = \beta_0 + \beta_1X_i + \varepsilon_i$$
- $Y_i|X_i \sim N\left(\beta_0 + \beta_1X_i , \sigma^2\right)$
- $E(Y_i|X_i) = \mu_{y|x} = \beta_0 + \beta_1X_i$
- $Var(Y|X_i ) = \sigma^2$

--- .class #id

## Picture of this


```{r, echo=FALSE}
n <- 10000
x <- seq(1,10, by =0.00090009)
y <- rnorm(n, 3 + 5*x, 1)
Data <- data.frame(x, y)

plot(x,y, type='n')
lines(lowess(y~x), )
segments(3,0,3,40)
points(3,40, pch=16)
points(3, 18, pch=16)
segments(-2,40, 3,40, lty=2)
segments(-2,18, 3,18, lty=2)
text(1.7 , 30, label= "Error E")
arrows(1.7, 30.7, 1.7, 40)
arrows(1.7, 29.3, 1.7, 18)
text(4,50, label= "Observed")
text(4,48, label = "value Y at X")
arrows(4,47, 3,40)
text(4, 15, label=expression(mu["Y|X"]))
arrows(3.7,15, 3.1, 18)
text(8, 30, label="Population Regression Line")
text(8,27, label= expression( mu["Y|X"] ~ "=" ~beta[0] + ~ beta[1]~X  ))
```


--- .class #id

## What Does This Tell Us?

- We can refer back to our scatter plot now and discuss what is the "best" line. 
- Given the previous image we can see that a good estimator would somehow have smaller residual errors. 
- So the "best" line would minimize the errors. 

--- .class #id


## Residual Errors


```{r, echo=FALSE}
plot(x,y, type='n')
lines(lowess(y~x))
segments(2, 20, 2, 13, lty=2)
points(2,20, pch=16)
segments(4, 10, 4, 23, lty=2)
points(4,10, pch=16)
segments(7, 45, 7, 38, lty=2)
points(7,45, pch=16)
segments(9, 32, 9, 48, lty=2)
points(9,32, pch=16)
text(1.6, 16, label = expression(hat(epsilon)[1]))
text(4.4, 16, label = expression(hat(epsilon)[2]))
text(6.6, 41, label = expression(hat(epsilon)[3]))
text(9.4, 41, label = expression(hat(epsilon)[4]))
```

--- .class #id


## In Comes Least Squares

- The least squares estimator of regression coefficients in the estimator that minimizes the sum of squared errors.
- We denote these estimators as $\hat{\beta}_0$ and $\hat{\beta}_1$. 
- In other words we attempt to minimize 
$$\sum_{i=1}^n \left(\varepsilon_i\right)^2 = \sum_{i=1}^n \left(Y_i - \hat{\beta}_0 - \hat{\beta}_1X_i\right)^2$$

--- .class #id



## Inferences on OLS

- Once we have our intercept and slope estimators the next step is to determine if they are significant or not. 
- Typically with hypothesis testing we have needed the following:
    - Population/Assumed Value of interest
    - Estimated value
    - Standard error of Estimate


--- .class #id

## Confidence Interval Creation

- with 95% confidence intervals of
$$\hat{\beta}_1 \pm t_{n-2, 0.975} \cdot se\left(\hat{\beta}_1\right)$$
$$\hat{\beta}_0 \pm t_{n-2, 0.975} \cdot se\left(\hat{\beta}_0\right)$$
- In general we can find a  $100(1-\alpha)\%$ confidence interval as
$$\hat{\beta}_1 \pm t_{n-2, 1-\dfrac{\alpha}{2}} \cdot se\left(\hat{\beta}_1\right)$$
$$\hat{\beta}_0 \pm t_{n-2, 1-\dfrac{\alpha}{2}} \cdot se\left(\hat{\beta}_0\right)$$




--- .class #id

## Example: Year and Appearances

```{r, eval=FALSE}

model <- lm(appearances~year, data=comic_characters)
tidy(model, conf.int=TRUE)[,-c(3:4)]
glance(model)
```

--- .class #id


## Example: Year and Appearances

```{r, echo=FALSE}

model <- lm(appearances~year, data=comic_characters)

glance(model)
```

--- .class #id


## Example: Year and Appearances

```{r, echo=FALSE}

model <- lm(appearances~year, data=comic_characters)

glance(model)
```

--- .class #id

## Interpreting the Coefficients: Continuous

- Before we can discuss the regression coefficients we need to understand how to interpret what these coefficients mean. 
-  $\beta_0$ is mean value for $Y$ when $X=0$.
- What about $\beta_1$?


--- .class #id

## Interpreting the Coefficients: Continuous

- Then we consider $\beta_1$ to see the meaning of this we do the following
$$
\begin{aligned}
    E(Y|X=x+1) - E(Y|X=x) &= \beta_0 + \beta_1(x+1) - \beta_0 - \beta_1x\\
    &= \beta_1
\end{aligned}
$$


--- .class #id


## Interpreting the Coefficients: Continuous

- We consider $\beta_0$ first. 
- Does this value have meaning with our current data? 
    - The estimated value of time level is only applicable to year within the range of our data. 
    - Many times the intercept is scientifically meaningless. 
    - Even if meaningless on its own, $\beta_0$ is necessary to specify the equation of our regression line. 
    - **Note:** People do sometimes use mean centered data and the intercept is then interpretable.



--- .class #id



## Interpreting the Coefficients: Continuous

- This gives us the interpretation that $\beta_1$ represents the mean change in outcome $Y$ given a one unit increase in predictor $X$. 
- This is not an actual prescription though, this is considering different subjects or groups of subjects who differ by one unit. 
- Below are correct interpretations of $\beta_1$ in our example. 
    - *These results display that the mean difference in appearances for a 1 year difference is -0.596*
    - *These results display that the mean difference in time for a 10 year difference is -5.96*


--- .class #id




## Multiple Regression

- We have been discussing simple models so far. 
- This works well when you have:
    - Randomized Data to test between specific groups (Treatment vs Control)
- In most situations we need look at more than just one relationship. 
- Think of this as needing more information to tell the entire story. 

--- .class #id



## Multiple Linear Regression with appearances



- First start with univariate models
- Then perform the multiple model





--- .class #id

## Multivariate Models


```{r}
mod3 <- lm(appearances~publisher + year, data=comic_characters)
tidy3 <- tidy(mod3, conf.int=T)[,-c(3:4)]
tidy3
```

--- .class #id

## Interpreting Multiple Coefficients

- The intercept is when all coefficients are zero. 
- Each other coefficient is interpreted in context to another. 


--- .class #id

## Interpreting Multiple Coefficients: Our Example

- Intercept: DC average appearances at year 0. ***Not Meaningful***
- Publisher Coefficient: If we consider 2 characters in the same year, the character from Marvel will have on average 9.54 less appearances than the character from DC. 
- Year Coefficient: If we consider 2 characters from the same publisher, an increase in 1 year will lead to on average 0.62 less appearances. 



--- .segue bg:grey


# Multiple Regression


--- .class #id

## Multiple Regression

- We have been discussing simple models so far. 
- This works well when you have:
    - Randomized Data to test between specific groups (Treatment vs Control)
- In most situations we need look at more than just one relationship. 
- Think of this as needing more information to tell the entire story. 


--- .class #id


## Motivating Example

- Health disparities are very real and exist across individuals and populations. 
- Before developing methods of remedying these disparities we need to be able to identify where there are disparities.In this homework we will consider a study by [(Asch & Armstrong, 2007)](http://www.ncbi.nlm.nih.gov/pubmed/17513818).  
- This paper considers 222 patients with localized prostate cancer. 


--- .class #id



## Motivating Example 

- The table below partitions patients by race, hospital and whether or not the patient received a prostatectomy. 

|       |   Race | Prostatectomy | No Prostatectomy | 
| -------------- | -------- | ---------- | ------------ |
University Hospital | White | 54 | 37 | 
|  | Black | 7 | 5  |
| VA Hospital | White | 11 | 29 | 
| | Black | 22 | 57 | 



--- .class #id






## Loading the Data


You can load this data into R with the code below:


```{r}
phil_disp <- read.table("https://drive.google.com/uc?export=download&id=0B8CsRLdwqzbzOXlIRl9VcjNJRFU", header=TRUE, sep=",")
```


--- .class #id



## The Data 

This dataset contains the following variables: 


| Variable |       Description | 
| ----------- | -------------------- | 
| hospital  |     0 - University Hospital |
|           |     1 - VA Hospital | 
| race   |      0 - White |
|      |        1 - Black | 
| surgery |       0 - No prostatectomy |
|          |    1 - Had Prostatectomy | 
| number    |    Count of people in Category |





--- .class #id

## Consider Prostatectomy by Race

```{r}
library(broom)
prost_race <- glm(surgery ~ race, weight=number, data= phil_disp,
                  family="binomial")
tidy(prost_race, exponentiate=T, conf.int=T)[,-c(3:4)]
```



--- .class #id



## Consider Prostatectomy by Race

- What can we conclude? 
- What kind of policy might we want to invoke based on this discovery?



--- .class #id


## Consider Prostatectomy by Hospital

```{r}
prost_hosp <- glm(surgery ~ hospital, weight=number, data= phil_disp,
                  family="binomial")
tidy(prost_hosp, exponentiate =T, conf.int=T)[,-c(3:4)]
```


--- .class #id


## Consider Prostatectomy by Hospital

- What can we conclude? 


--- .class #id

## Multiple Regression of Prostatectomy


```{r}
prost <- glm(surgery ~ hospital + race, weight=number, data= phil_disp,
             family="binomial")
tidy(prost, exponentiate=T, conf.int=T)[,-c(3:4)]
```




--- .class #id


## Multiple Regression of Prostatectomy

- What can We conclude?
- What happened here?
- Does this change our policy suggestion from before?




--- .class #id


## Benefits of Multiple Regression


- Multiple Regression helps us tell a more complete story. 
- Multiple regression controls for confounding. 




--- .class #id


## Confounding

- Associated with both the Exposure and the Outcome
- Even if the Exposure and Outcome are not related, unmeasured confounding can show that they are. 



--- .class #id



## What Do We Do with Confounding?

- We must add all confounders into our model. 
- Without adjusting for confounders are results may be highly biased. 
- Without adjusting for confounding we may make incorrect policies that do not fix the problem. 



--- .class #id


## Multiple Linear Regression with appearances



- First start with univariate models
- Then perform the multiple model






--- .class #id


## Multivariate Models


```{r}
library(broom)
library(fivethirtyeight)
mod3 <- lm(appearances~publisher + year, data=comic_characters)
tidy3 <- tidy(mod3, conf.int=T)[,-c(3:4)]
tidy3
```



--- .class #id

## Interpreting Multiple Coefficients

- The intercept is when all coefficients are zero. 
- Each other coefficient is interpreted in context to another. 




--- .class #id


## Interpreting Multiple Coefficients: Our Example

- Intercept: DC average appearances at year 0. ***Not Meaningful***
- Publisher Coefficient: If we consider 2 characters in the same year, the character from Marvel will have on average 9.54 less appearances than the character from DC. 
- Year Coefficient: If we consider 2 characters from the same publisher, an increase in 1 year will lead to on average 0.62 less appearances. 



--- .class #id

## Further Example: Bike Sharing Data


- We have hourly data spanning 2 years
- This dataset has the first 19 days of each month. 
- Goal is to find the total count of bike rented. 


--- .class #id


## Further Example: Bike Sharing Data

| Data |  Fields |
| :------: | :----------------------|
| datetime | hourly date + timestamp  |
| season |   1 = spring, 2 = summer, 3 = fall, 4 = winter  |
| holiday |  whether the day is considered a holiday |
| workingday |  whether the day is neither a weekend nor holiday |


--- .class #id


## Further Example: Bike Sharing Data

| Data |  Fields |
| :------: | :----------------------|
| weather |  1: Clear, Few clouds, Partly cloudy, Partly cloudy  |
| | 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist | 
| | 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds |
| | 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog |
| temp |  temperature in Celsius |


--- .class #id


## Further Example: Bike Sharing Data

| Data |  Fields |
| :------: | :----------------------|
| atemp |  "feels like" temperature in Celsius |
| humidity |  relative humidity |
| windspeed |  wind speed|
| casual |  number of non-registered user rentals initiated |
| registered |  number of registered user rentals initiated |
| count |  number of total rentals |


--- .class #id

## Univariate Regressions

```{r}
library(readr)
library(tidyverse)
bikes <- read_csv("../Notes/Data/bike_sharing.csv") %>%
        mutate(season = as.factor(season)) %>%
        mutate(weather=as.factor(weather)) 
       
```


--- .class #id

## Univariate Regressions

```{r}
mod1 <- lm(count~season, data=bikes)
mod2 <- lm(count~holiday, data=bikes)
mod3 <- lm(count~workingday, data=bikes)
mod4 <- lm(count~weather, data=bikes)
mod5 <- lm(count~temp, data=bikes)
mod6 <- lm(count~atemp, data=bikes)
mod7 <- lm(count~humidity, data=bikes)
mod8 <- lm(count~windspeed, data=bikes)
mod9 <- lm(count~casual, data=bikes)
mod10 <- lm(count~registered, data=bikes)
```


--- .class #id

## Univariate Regressions

```{r, eval=F}
library(broom)
tidy1 <- tidy( mod1, conf.int=T )[-1, -c(3:4) ]
tidy2 <- tidy(mod2, conf.int=T )[-1, -c(3:4) ]
tidy3 <- tidy(mod3 , conf.int=T)[-1, -c(3:4) ]
tidy4 <- tidy(mod4 , conf.int=T)[-1, -c(3:4) ]
tidy5 <- tidy(mod5, conf.int=T)[-1, -c(3:4) ]
tidy6 <- tidy(mod6 , conf.int=T)[-1, -c(3:4) ]
tidy7 <- tidy(mod7 , conf.int=T)[-1, -c(3:4) ]
tidy8 <- tidy(mod8 , conf.int=T)[-1, -c(3:4) ]
tidy9 <- tidy(mod9, conf.int=T)[-1, -c(3:4) ]
tidy10 <- tidy(mod10, conf.int=T)[-1, -c(3:4) ]
bind_rows(tidy1, tidy2, tidy3, tidy4, tidy5, tidy6, tidy7, tidy8, tidy9, tidy10) 

```



--- .class #id

## Univariate Regressions

```{r, echo=F}
library(broom)
tidy1 <- tidy( mod1, conf.int=T )[-1, -c(3:4) ]
tidy2 <- tidy(mod2, conf.int=T )[-1, -c(3:4) ]
tidy3 <- tidy(mod3 , conf.int=T)[-1, -c(3:4) ]
tidy4 <- tidy(mod4 , conf.int=T)[-1, -c(3:4) ]
tidy5 <- tidy(mod5, conf.int=T)[-1, -c(3:4) ]
tidy6 <- tidy(mod6 , conf.int=T)[-1, -c(3:4) ]
tidy7 <- tidy(mod7 , conf.int=T)[-1, -c(3:4) ]
tidy8 <- tidy(mod8 , conf.int=T)[-1, -c(3:4) ]
tidy9 <- tidy(mod9, conf.int=T)[-1, -c(3:4) ]
tidy10 <- tidy(mod10, conf.int=T)[-1, -c(3:4) ]
rbind(tidy1, tidy2, tidy3, tidy4, tidy5, tidy6, tidy7, tidy8, tidy9, tidy10)
```


--- .class #id


## Multivariate

```{r, eval=F}
mod.final <- lm(count~season+weather+humidity+windspeed, data=bikes)
tidy(mod.final)[-1,-c(3:4)]
glance(mod.final)
```

--- .class #id

## Multivariate

```{r, echo=F}
mod.final <- lm(count~season+weather+humidity+windspeed, data=bikes)
tidy(mod.final)[-1,-c(3:4)]
```

--- .class #id


## Multivariate

```{r, echo=F}
glance(mod.final)
```